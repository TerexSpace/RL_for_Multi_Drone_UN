# CIGRL Training Configuration
# Multi-Agent PPO with Attention-based Communication

training:
  algorithm: "MAPPO"
  total_timesteps: 10_000_000
  n_envs: 8
  batch_size: 2048
  minibatch_size: 256
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  learning_rate: 3.0e-4
  lr_schedule: "linear"

network:
  state_dim: 15
  action_dim: 3
  hidden_dim: 128
  neighbor_dim: 12
  max_neighbors: 5
  attention_heads: 4
  use_attention: true
  use_communication: true

environment:
  num_drones: 5
  urban_size: 1000.0
  dt: 0.1
  max_steps: 500
  gps_denied_zones: 3
  comm_range: 150.0
  comm_failure_prob: 0.1

reward:
  goal_weight: 1.0
  collision_penalty: -10.0
  near_miss_penalty: -1.0
  gps_coop_bonus: 0.5
  smooth_control_weight: 0.1
  formation_weight: 0.2

evaluation:
  eval_freq: 50000
  n_eval_episodes: 20
  save_best: true
  log_interval: 1000

checkpoints:
  save_freq: 100000
  keep_last: 5
  save_path: "models/checkpoints"
  best_model_path: "models/best_model.pt"

logging:
  wandb_project: "cigrl"
  wandb_entity: null
  tensorboard: true
  log_dir: "logs"
